{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dctime/minecraft-texture-generation/blob/main/sword_diffusion_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qW1Ihc_V8kRk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as dset\n",
        "import shutil\n",
        "import os\n",
        "import numpy\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEl31--045Rf",
        "outputId": "7b2ac9fc-2d5c-408f-c051-536165d08e75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.unpack_archive(\"drive/MyDrive/Minecraft Sword Generator/minecraft_textures.zip\", \"\")"
      ],
      "metadata": {
        "id": "vC-g3LlHDRHh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        image = Image.open(f).convert(\"RGBA\")\n",
        "        new_image = Image.new(\"RGBA\", image.size, \"WHITE\")\n",
        "        new_image.paste(image, mask=image)\n",
        "        return new_image.convert(\"RGB\")\n",
        "        # return image\n",
        "# dataset = datasets.ImageFolder(root=\"minecraft_textures\")\n",
        "dataset = datasets.ImageFolder(root=\"minecraft_textures\", loader=custom_loader)\n",
        "\n",
        "IMAGE_CHANNELS = 3"
      ],
      "metadata": {
        "id": "eYR-AwSAFoif"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "plt.imshow(dataset[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "6MEwv_7rFEMg",
        "outputId": "c0c148f7-e9dd-4b97-b760-b1474f0132c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 2240\n",
            "    Root location: minecraft_textures\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e94602a5150>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise the image"
      ],
      "metadata": {
        "id": "9bEG1BICWK9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.03):\n",
        "    return torch.linspace(start, end, timesteps)\n",
        "\n",
        "def get_index_from_list(vals, t, x_shape):\n",
        "    \"\"\"\n",
        "    Returns a specific index t of a passed list of values vals\n",
        "    while considering the batch dimension.\n",
        "    \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Takes an image and a timestep as input and\n",
        "    returns the noisy version of it\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(x_0)\n",
        "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
        "    )\n",
        "    # mean + variance\n",
        "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
        "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
        "\n",
        "\n",
        "# Define beta schedule\n",
        "T = 300\n",
        "betas = linear_beta_schedule(timesteps=T)\n",
        "\n",
        "# Pre-calculate different terms for closed form\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
      ],
      "metadata": {
        "id": "hSvV7o5lWOUQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 64\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "def load_transformed_dataset():\n",
        "    data_transforms = [\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(), # Scales data into [0,1]\n",
        "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
        "    ]\n",
        "    data_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    dataset = datasets.ImageFolder(root=\"minecraft_textures\", loader=custom_loader, transform=data_transform)\n",
        "    # dataset = datasets.ImageFolder(root=\"minecraft_textures\", transform=data_transform)\n",
        "\n",
        "    return dataset\n",
        "def show_tensor_image(image):\n",
        "    reverse_transforms = transforms.Compose([\n",
        "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "        transforms.Lambda(lambda t: t * 255.),\n",
        "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "\n",
        "    # Take first image of batch\n",
        "    if len(image.shape) == 4:\n",
        "        image = image[0, :, :, :]\n",
        "    plt.imshow(reverse_transforms(image))\n",
        "\n",
        "data = load_transformed_dataset()\n",
        "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "FFbF0Hm6b4Uu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate forward diffusion\n",
        "image = next(iter(dataloader))[0]\n",
        "\n",
        "plt.figure(figsize=(IMG_SIZE, IMG_SIZE))\n",
        "plt.axis('off')\n",
        "num_images = 10\n",
        "stepsize = int(T/num_images)\n",
        "\n",
        "for idx in range(0, T, stepsize):\n",
        "    t = torch.Tensor([idx]).type(torch.int64)\n",
        "    plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n",
        "    img, noise = forward_diffusion_sample(image, t)\n",
        "    show_tensor_image(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLpF3-FYc1HB",
        "outputId": "2c9db77a-cafe-420b-eff9-bf47f12f4ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-59ea63135d7b>:11: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
            "  plt.subplot(1, num_images+1, int(idx/stepsize) + 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Denoise Model"
      ],
      "metadata": {
        "id": "BBfQRuHZnEBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import xor\n",
        "from torch import nn\n",
        "import math\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
        "        super().__init__()\n",
        "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
        "        if up:\n",
        "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1) # stride = 2 => img size *(2) cuz padding = 1\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1) # stride = 2 => img size *(1/2) cuz padding = 1\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu  = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, t, ):\n",
        "        # print(\"before:\", x.shape)\n",
        "        # First Conv\n",
        "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
        "        # print(\"step1\", h.shape)\n",
        "        # Time embedding\n",
        "        time_emb = self.relu(self.time_mlp(t))\n",
        "        # Extend last 2 dimensions\n",
        "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
        "        # Add time channel\n",
        "        h = h + time_emb\n",
        "        # print(\"step2\", h.shape)\n",
        "        # Second Conv\n",
        "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
        "        # Down or Upsample\n",
        "        # print(\"step3:\", h.shape)\n",
        "        h = self.transform(h)\n",
        "        # print(\"after:\", h.shape)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        # TODO: Double check the ordering here\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class SimpleUnet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified variant of the Unet architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        image_channels = IMAGE_CHANNELS\n",
        "        down_channels = (32, 64, 128, 256, 512)\n",
        "        up_channels = (512, 256, 128, 64, 32)\n",
        "        out_dim = IMAGE_CHANNELS\n",
        "        time_emb_dim = 32\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(time_emb_dim), # input: float output: vector floats -1 to 1\n",
        "                nn.Linear(time_emb_dim, time_emb_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # Initial projection\n",
        "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
        "\n",
        "        # Downsample\n",
        "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], time_emb_dim) for i in range(len(down_channels)-1)])\n",
        "        # Upsample\n",
        "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], time_emb_dim, up=True) for i in range(len(up_channels)-1)])\n",
        "\n",
        "        # Edit: Corrected a bug found by Jakub C (see YouTube comment)\n",
        "        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n",
        "\n",
        "    def forward(self, x, timestep): # input noised img and timestep\n",
        "        # Embedd time\n",
        "        t = self.time_mlp(timestep)\n",
        "        # Initial conv\n",
        "        x = self.conv0(x)\n",
        "        # Unet\n",
        "        residual_inputs = []\n",
        "        for down in self.downs:\n",
        "            x = down(x, t)\n",
        "            residual_inputs.append(x)\n",
        "        for up in self.ups:\n",
        "            residual_x = residual_inputs.pop()\n",
        "            # Add residual x as additional channels\n",
        "            # print(residual_x.shape, x.shape)\n",
        "            x = torch.cat((x, residual_x), dim=1)\n",
        "            x = up(x, t)\n",
        "        return self.output(x)\n",
        "\n",
        "model = SimpleUnet()\n",
        "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
        "model"
      ],
      "metadata": {
        "id": "DrtGK3XanGn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss(model, x_0, t):\n",
        "    x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
        "    noise_pred = model(x_noisy, t)\n",
        "    return F.l1_loss(noise, noise_pred)"
      ],
      "metadata": {
        "id": "pT3hVZJW6Ccd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_timestep(x, t):\n",
        "    \"\"\"\n",
        "    Calls the model to predict the noise in the image and returns\n",
        "    the denoised image.\n",
        "    Applies noise to this image, if we are not in the last step yet.\n",
        "    \"\"\"\n",
        "    betas_t = get_index_from_list(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
        "\n",
        "    # Call model (current image - noise prediction)\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
        "\n",
        "    if t == 0:\n",
        "        # As pointed out by Luis Pereira (see YouTube comment)\n",
        "        # The t's are offset from the t's in the paper\n",
        "        return model_mean\n",
        "    else:\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_plot_image():\n",
        "    # Sample noise\n",
        "    img_size = IMG_SIZE\n",
        "    img = torch.randn((1, IMAGE_CHANNELS, img_size, img_size), device=device)\n",
        "    plt.figure(figsize=(32,32))\n",
        "    plt.axis('off')\n",
        "    num_images = 10\n",
        "    stepsize = int(T/num_images)\n",
        "\n",
        "    for i in range(0,T)[::-1]:\n",
        "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
        "        img = sample_timestep(img, t)\n",
        "        # Edit: This is to maintain the natural range of the distribution\n",
        "        img = torch.clamp(img, -1.0, 1.0)\n",
        "        if i % stepsize == 0:\n",
        "            plt.subplot(1, num_images, int(i/stepsize)+1)\n",
        "            show_tensor_image(img.detach().cpu())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MyTik_tKKv_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "epochs = 1000 # Try more!\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
        "      loss = get_loss(model, batch[0], t)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if epoch % 5 == 0 and step == 0:\n",
        "        print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
        "        sample_plot_image()"
      ],
      "metadata": {
        "id": "KQ0k809G_wIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
        "\n",
        "for i in range(0,T)[::-1]:\n",
        "  t = torch.full((1,), i, device=device, dtype=torch.long)\n",
        "  img = sample_timestep(img, t)\n",
        "  img = torch.clamp(img, -1.0, 1.0)\n",
        "\n",
        "show_tensor_image(img.detach().cpu())"
      ],
      "metadata": {
        "id": "n8ABDiAZPsfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'model_64.pth')"
      ],
      "metadata": {
        "id": "_FL9vexRsN5K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}